# ğŸ“˜ Note AI

> **A Document-Based Question Answering (RAG) System built with FastAPI, LangChain, and React.**

**Note AI** allows you to upload multiple documents (PDF, DOCX, TXT), extract knowledge, and ask natural language questions about the content. The backend retrieves relevant chunks using vector search and generates accurate answers using Large Language Models (LLMs).

---

## ğŸ“¸ Screenshots

| Dashboard | Q&A Interface | Chat History | Download Button | Downloaded PDF Preview |
|:---:|:---:|:---:|:---:|:---:|
| ![Dashboard](Client/src/assets/SS1.png) | ![QnA](Client/src/assets/SS2.png) | ![History](Client/src/assets/SS3.png) |![Download Button](Client/src/assets/SS4.png) | ![Downloaded PDF](Client/src/assets/SS5.png) |

---

## ğŸš€ Features

### ğŸ”¹ Frontend (React + Vite)
* **Clean UI:** Modern interface for uploading PDF, DOCX, or TXT files.
* **Interactive Chat:** Real-time chat-style Q&A interface.
* **History Tracking:** Sidebar displays chat history and previous model responses.
* **Feedback:** Loading indicators, error handling, and smooth icons (React Icons).
* **Configurable:** Environment-based API configuration.

### ğŸ”¹ Backend (FastAPI + LangChain)
* **Document Parsing:** Robust file upload endpoint supporting PDF & DOCX text extraction.
* **Vector Search:** Embedding generation and indexing using **FAISS** or **Chroma**.
* **RAG Pipeline:** Retrieval-Augmented Generation for accurate, context-aware answers.
* **Contextual Memory:** Chat history support for follow-up queries.
* **Model Flexibility:** Support for custom models (Mistral, Llama, Qwen, Zephyr).

---

## ğŸ¤– Supported Models

Note AI leverages Hugging Face models for embeddings and text generation:

* **Embeddings:** `sentence-transformers/all-MiniLM-L6-v2`
* **LLM:** `HuggingFaceH4/zephyr-7b-beta`

---

## ğŸ“‚ Project Structure

```text
Note AI/
â”‚
â”œâ”€â”€ ğŸ“ Client/                  # Frontend (React + Vite)
â”‚   â”œâ”€â”€ ğŸ“‚ public/
â”‚   â”œâ”€â”€ ğŸ“‚ src/
â”‚   â”‚   â”œâ”€â”€ ğŸ–¼ï¸ assets/
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“¸ screenshots/
â”‚   â”‚   â”œâ”€â”€ ğŸ§© components/
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ FileUpload.jsx
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ QnA.jsx
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ ChatHistoryDisplay.jsx
â”‚   â”‚   â”œâ”€â”€ âš›ï¸ App.jsx
â”‚   â”‚   â”œâ”€â”€ ğŸ¨ App.css
â”‚   â”‚   â”œâ”€â”€ âš›ï¸ main.jsx
â”‚   â”‚   â”œâ”€â”€ ğŸ¨ index.css
â”‚   â”‚   â””â”€â”€ ğŸ¨ QnA.css
â”‚   â”œâ”€â”€ âš™ï¸ vite.config.js
â”‚   â”œâ”€â”€ ğŸ§¾ package.json
â”‚   â””â”€â”€ ğŸŒ index.html
â”‚
â””â”€â”€ ğŸ–¥ï¸ Server/                  # Backend (FastAPI)
    â”œâ”€â”€ ğŸ main.py
    â”œâ”€â”€ ğŸ§  rag_pipeline.py
    â”œâ”€â”€ ğŸ“¦ models/
    â”œâ”€â”€ ğŸ“‚ uploads/
    â”œâ”€â”€ ğŸ§  vectorstore/
    â”œâ”€â”€ ğŸ§¾ requirements.txt
    â””â”€â”€ ğŸ”’ .env
```
ğŸ› ï¸ Installation & Setup
Follow these steps to run the project locally.

1. Clone the repository

git clone [https://github.com/your-username/note-ai.git](https://github.com/your-username/note-ai.git)
cd note-ai

# ğŸ§  Backend Setup (FastAPI)
Navigate to the server directory and set up the Python environment.

## 1. Navigate to Server
cd Server

## 2. Create virtual environment
python -m venv venv

## 3. Activate virtual environment
### Mac/Linux:
source venv/bin/activate
### Windows:
venv\Scripts\activate

## 4. Install dependencies
pip install -r requirements.txt

## 5. Setup Environment Variables
 Create a .env file in the Server directory and add your keys (if required by specific models)

 HUGGINGFACEhub_API_TOKEN=your_token_here

## 6. Run the server
uvicorn main:app --reload --host 0.0.0.0 --port 8000
The Backend will be running at: http://localhost:8000

# ğŸ’» Frontend Setup (React)
Open a new terminal, navigate to the client directory, and start the UI.


## 1. Navigate to Client
cd ../Client

## 2. Install dependencies
npm install

## 3. Configure Environment
Create a .env file in the Client directory:
echo "VITE_RENDER_API_URL=http://localhost:8000"  .env

## 4. Run the app
npm run dev<br>
The Frontend will be running at: http://localhost:5173

# ğŸ§ª Usage Workflow
Start the Backend: Ensure the FastAPI server is running.

Start the Frontend: Ensure the React app is running.

Upload Documents: Use the "Upload" button to select PDF, DOCX, or TXT files.

Processing: The system will extract text, create embeddings, and store them in the vector database.

Ask Questions: Type your query in the chat input.

Get Answers: The AI will retrieve relevant context and generate an answer with sources displayed below.

# ğŸ¤ Contributing
Contributions are welcome! Please fork the repository and create a pull request.

# ğŸ“„ License
MIT License
