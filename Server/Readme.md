# ğŸ“˜ Note AI
A Document-Based Question Answering (RAG) System built with **FastAPI**, **LangChain**, and a modern **React** frontend.

Note AI allows you to upload multiple documents, extract knowledge, and ask natural language questions.  
The backend retrieves relevant chunks using vector search and generates accurate answers using LLMs.

---

## ğŸš€ Features

### ğŸ”¹ Frontend (React + Vite)
- Clean UI for uploading PDF, DOCX, or TXT files
- Real-time chat-style Q&A interface
- Shows chat history and model responses
- Loading indicators & error handling
- Smooth icons using React Icons
- Environment-based API configuration

### ğŸ”¹ Backend (FastAPI + LangChain)
- File upload endpoint with document parsing
- PDF & DOCX text extraction
- Embedding generation
- Vector store indexing (FAISS or Chroma)
- RAG pipeline with retrieval + generation
- Chat history support for contextual queries
- Custom model selection (Mistral, Llama, Qwen, etc.)

---

## ğŸ“‚ Project Structure

Note AI/
â”‚
â”œâ”€â”€ ğŸ“ Client/
â”‚   â”œâ”€â”€ ğŸ“‚ public/
â”‚   â”œâ”€â”€ ğŸ“‚ src/
â”‚   â”‚   â”œâ”€â”€ ğŸ–¼ï¸ assets/
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“¸ screenshots/
â”‚   â”‚   â”œâ”€â”€ ğŸ§© components/
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ FileUpload.jsx
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ QnA.jsx
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ ChatHistoryDisplay.jsx
â”‚   â”‚   â”œâ”€â”€ âš›ï¸ App.jsx
â”‚   â”‚   â”œâ”€â”€ ğŸ¨ App.css
â”‚   â”‚   â”œâ”€â”€ âš›ï¸ main.jsx
â”‚   â”‚   â”œâ”€â”€ ğŸ¨ index.css
â”‚   â”‚   â””â”€â”€ ğŸ¨ QnA.css
â”‚   â”œâ”€â”€ âš™ï¸ vite.config.js
â”‚   â”œâ”€â”€ ğŸ§¾ package.json
â”‚   â””â”€â”€ ğŸŒ index.html
â”‚
â”œâ”€â”€ ğŸ–¥ï¸ Server/
â”‚   â”œâ”€â”€ ğŸ main.py
â”‚   â”œâ”€â”€ ğŸ§  rag_pipeline.py
â”‚   â”œâ”€â”€ ğŸ“¦ models/
â”‚   â”œâ”€â”€ ğŸ“‚ uploads/
â”‚   â”œâ”€â”€ ğŸ§  vectorstore/
â”‚   â”œâ”€â”€ ğŸ§¾ requirements.txt
â”‚   â””â”€â”€ ğŸ”’ .env
â”‚
â””â”€â”€ ğŸš« .gitignore




---

## ğŸ› ï¸ Installation & Setup

### 1. Clone the repository

git clone https://github.com/your-username/note-ai.git
cd note-ai

ğŸ§  Backend Setup (FastAPI)
2. Create virtual environment
cd backend
python -m venv venv
source venv/bin/activate   # Mac/Linux
venv\Scripts\activate      # Windows

3. Install dependencies
pip install -r requirements.txt

4. Run the FastAPI server
uvicorn main:app --reload --host 0.0.0.0 --port 8000


Backend runs at:
ğŸ‘‰ http://localhost:8000

ğŸ’» Frontend Setup (React + Vite)
5. Install dependencies
cd ../frontend
npm install

6. Create environment file
VITE_RENDER_API_URL=http://localhost:8000

7. Run app
npm run dev


Frontend runs at:
ğŸ‘‰ http://localhost:5173

ğŸ§ª Usage Workflow

Start backend

Start frontend

Upload a PDF, DOCX, or TXT file

System extracts text + stores embeddings

Ask questions in the chatbox

System retrieves relevant text and generates an answer

Sources are displayed under the answer

ğŸ¤– Supported LLM Models

UseD Hugging Face model for text-generation:

sentence-transformers/all-MiniLM-L6-v2
HuggingFaceH4/zephyr-7b-beta

![Screenshot](Client/src/assets/screenshots/SS1.png)
![Screenshot](Client/src/assets/screenshots/SS2.png)
![Screenshot](Client/src/assets/screenshots/SS3.png)